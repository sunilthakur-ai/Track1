{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunilthakur-ai/Track1/blob/master/Building_Multimodal_AI_Application_with_Gemini_2_0_Pro_SOLUTION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gC3oo3k1g2RZ"
      },
      "source": [
        "## Note:\n",
        "\n",
        "1. To replicate this walkthrough — first download the [files from this folder](https://drive.google.com/drive/folders/1mV7Q4K9WzUfCW9GkMjooJSw2vPq6_jbO?usp=sharing)\n",
        "2. Upload them to colab under files\n",
        "\n",
        "## Install Required Packages\n",
        "\n",
        "To get started, install the necessary Python packages. These include:\n",
        "\n",
        "- `google-generativeai`: The official SDK for interacting with Gemini 2.0.\n",
        "- `gradio`: A low-code framework for building interactive web apps.\n",
        "- `yt-dlp`: A tool for downloading YouTube videos.\n",
        "- `wget`: A utility for retrieving files from the web."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPhCn6JpWjkk",
        "outputId": "5c2b6a47-9b6a-425e-e36b-c981fc204928"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.16.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.8-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.7.0 (from gradio)\n",
            "  Downloading gradio_client-1.7.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.5)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.9.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.0->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.0->gradio) (14.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.16.0-py3-none-any.whl (62.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.2/62.2 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.7.0-py3-none-any.whl (321 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.9/321.9 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.8-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.9.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m109.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 fastapi-0.115.8 ffmpy-0.5.0 gradio-5.16.0 gradio-client-1.7.0 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.20 ruff-0.9.6 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.45.3 tomlkit-0.13.2 uvicorn-0.34.0\n",
            "Collecting yt_dlp\n",
            "  Downloading yt_dlp-2025.1.26-py3-none-any.whl.metadata (172 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yt_dlp-2025.1.26-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: yt_dlp\n",
            "Successfully installed yt_dlp-2025.1.26\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=b814d2ddf2feeac6884353f60296f6096d314c706fb4171389336a70a9e19540\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/b3/0f/a40dbd1c6861731779f62cc4babcb234387e11d697df70ee97\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install google-genai # The official SDK for interacting with Gemini 2.0\n",
        "!pip install gradio # A low-code framework for building interactive web apps\n",
        "!pip install yt_dlp # A tool for downloading YouTube videos\n",
        "!pip install wget # A utility for retrieving files from the web"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_lw1B6ng7NU"
      },
      "source": [
        "## Import Required Libraries\n",
        "\n",
        "Next, import the necessary libraries for working with Gemini 2.0 and building the multimodal application:\n",
        "\n",
        "- `google.genai`: The official library for interacting with Gemini models.\n",
        "- `PIL.Image`: Used for handling image data.\n",
        "- `IPython.display`: Enables rich media display (Markdown, HTML, Images).\n",
        "- `google.colab.userdata`: Helps manage Colab user authentication.\n",
        "- `os`: Provides access to system functionalities.\n",
        "- `pathlib`: Manages file directories.\n",
        "- `gradio`: Manages web interface.\n",
        "- `yt_dlp`: Lets us work with YouTube videos.\n",
        "- `wget`: Lets us download videos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fgSCTv_fB3G",
        "outputId": "fc6ad5e8-7ae1-4c08-f9f8-29e4e597d0ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:502: UserWarning: <built-in function any> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "# Gemini related packages\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "# Helper packages\n",
        "import time # Time some operations\n",
        "import os # Provides access to system functionalities\n",
        "import PIL.Image # Used for handling image data\n",
        "from IPython.display import Markdown, HTML, Image, display # Rich media display in a notebook\n",
        "from google.colab import userdata # Manage colab user authentication\n",
        "import pathlib # Manages file directories\n",
        "import gradio as gr # Manages web interface\n",
        "import yt_dlp # Lets us work with YouTube videos\n",
        "import wget # Lets us download videos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8Y8YX7zg_qR"
      },
      "source": [
        "## Getting Started with Gemini API\n",
        "\n",
        "To use Gemini 2.0, you need access to Google's Generative AI API. Follow these steps:\n",
        "\n",
        "### Step 1: Get API Access\n",
        "1. Visit the [Google AI Studio](https://aistudio.google.com/) and sign in with your Google account.\n",
        "2. Navigate to the **API Keys** section and generate a new API key.\n",
        "3. Copy the key and store it securely.\n",
        "\n",
        "### Step 2: Set Up the API Key in Your Notebook\n",
        "The code below retrieves the API key from Colab's `userdata` storage. If you're running this locally, store the API key as an environment variable or use a `.env` file.\n",
        "\n",
        "```python\n",
        "API_KEY = userdata.get('GOOGLE_GEMINI_API_KEY')\n",
        "client = genai.Client(api_key=API_KEY)\n",
        "```\n",
        "\n",
        "### Step 3: Understand the Gemini Ecosystem\n",
        "\n",
        "| Model Variant                     | Input(s)                     | Output          | Optimized For |\n",
        "|------------------------------------|------------------------------|----------------|---------------|\n",
        "| **Gemini 2.0 Flash** (`gemini-2.0-flash`) | Audio, images, videos, text | Text, images (coming soon), audio (coming soon) | Next-gen features, speed, and multimodal generation |\n",
        "| **Gemini 2.0 Flash-Lite Preview** (`gemini-2.0-flash-lite-preview-02-05`) | Audio, images, videos, text | Text | Cost efficiency and low latency |\n",
        "| **Gemini 1.5 Flash** (`gemini-1.5-flash`) | Audio, images, videos, text | Text | Fast and versatile performance |\n",
        "| **Gemini 1.5 Flash-8B** (`gemini-1.5-flash-8b`) | Audio, images, videos, text | Text | High volume, lower intelligence tasks |\n",
        "| **Gemini 1.5 Pro** (`gemini-1.5-pro`) | Audio, images, videos, text | Text | Complex reasoning and intelligence tasks |\n",
        "| **Gemini 1.0 Pro** (`gemini-1.0-pro`) *(Deprecated on 2/15/2025)* | Text | Text | Natural language tasks, multi-turn chat, code generation |\n",
        "| **Text Embedding** (`text-embedding-004`) | Text | Text embeddings | Measuring relatedness of text strings |\n",
        "| **AQA** (`aqa`) | Text | Text | Providing source-grounded answers |\n",
        "| **gemini-2.0-pro-exp-02-05**             | All                    | Text        | Improved quality, especially for world knowledge, code, and long context |\n",
        "| **gemini-2.0-flash-thinking-exp-01-21**  | All        | Text  | Reasoning for complex problems, features new thinking capabilities\n",
        "| **gemini-2.0-flash-exp**                 | All            | Text, images | Next generation features, superior speed, native tool use, and multimodal generation |\n",
        "| **gemini-exp-1206**                      | All                    | Text        | Quality improvements, celebrates 1 year of Gemini |\n",
        "| **learnlm-1.5-pro-experimental**         | Audio, images, videos, text | Text  | Multimodal learning and generation |\n",
        "\n",
        "\n",
        "\n",
        "[Source](https://ai.google.dev/gemini-api/docs/models/gemini?authuser=1)\n",
        "\n",
        "### Running Your First Prompt\n",
        "\n",
        "The `generate_content_stream()` function is used to generate text responses from the Gemini model in a **streaming** fashion. This means the model outputs text incrementally rather than waiting for the entire response before displaying it.\n",
        "\n",
        "#### Parameters:\n",
        "- **`model`** *(str)*: The specific version of the Gemini model to use.  \n",
        "- **`contents`** *(list[str])*: A list of input prompts or queries for the model.  \n",
        "- *(Optional parameters like temperature, top-k, top-p, and safety settings can also be adjusted, but they are not shown in this basic example, but will be explored later on.)*\n",
        "\n",
        "#### Streaming vs. Non-Streaming\n",
        "- **Streaming (`generate_content_stream`)**: Outputs text in real time, useful for chat-based applications or interactive experiences.  \n",
        "- **Non-Streaming (`generate_content`)**: Waits until the full response is generated before displaying it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOmn70opWxaA"
      },
      "outputs": [],
      "source": [
        "# Define model_ID\n",
        "model_id=\"gemini-2.0-pro-exp-02-05\" # Set this to gemini-2.0-flash for faster inference time\n",
        "\n",
        "# Retrieve the API key from Google Colab's user data storage\n",
        "API_KEY = userdata.get('GOOGLE_GEMINI_API_KEY')\n",
        "\n",
        "# Initialize the Gemini API client\n",
        "client = genai.Client(api_key=API_KEY)\n",
        "\n",
        "# Generate a response using the Gemini 2.0 model in streaming mode\n",
        "response = client.models.generate_content_stream(\n",
        "    model=model_id,  # Specify the Gemini model version\n",
        "    contents=[\"Explain how the stock market works\"]  # Provide the input prompt as a list\n",
        ")\n",
        "\n",
        "# Iterate over the streamed response and print each chunk as it arrives\n",
        "for chunk in response:\n",
        "    print(chunk.text, end=\"\")  # Print the text output without extra line breaks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-x5SDDEhMNE"
      },
      "source": [
        "## Overview of Helper Functions\n",
        "\n",
        "Before diving into building our multimodal application, let's explore some essential helper functions that will improve our interaction with Gemini 2.0. These functions allow us to:\n",
        "\n",
        "1. **Count Tokens**: Estimate the number of tokens a prompt will consume.\n",
        "2. **Modify Generation Parameters**: Control response creativity, length, and structure.\n",
        "3. **Use Safety Filters**: Apply content moderation to filter inappropriate responses.\n",
        "4. **Start a Multi-Turn Chat**: Maintain context and generate sequential responses.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiRJO7Zo0IxE"
      },
      "source": [
        "### Counting Tokens\n",
        "\n",
        "Before sending a prompt, it's useful to check how many tokens it consumes. This helps manage token limits and optimize responses. We can do this with `client.models.count_tokens()`\n",
        "\n",
        "```python\n",
        "response = client.models.count_tokens(\n",
        "    model=model_id,\n",
        "    contents=\"INSERT YOUR PROMPT HERE\",\n",
        ")\n",
        "print(response)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1xanZJSheen"
      },
      "outputs": [],
      "source": [
        "# Count how many tokens there are in the prompt \"What's the highest mountain in Africa?\"\n",
        "response = client.models.count_tokens(\n",
        "    model=model_id,\n",
        "    contents=\"What's the highest mountain in Africa?\",\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgeyHiSA0j2u"
      },
      "source": [
        "### Modifying model behavior\n",
        "\n",
        "We can adapt Gemini’s response behavior using generation parameters such as:\n",
        "\n",
        "- `temperature`: Controls randomness (lower = more deterministic).\n",
        "- `top_p` and `top_k`: Adjust response diversity.\n",
        "- `max_output_tokens`: Limits response length.\n",
        "- `stop_sequences`: Prevents responses beyond certain words.\n",
        "\n",
        "```python\n",
        "response = client.models.generate_content(\n",
        "    model=model_id,  # Specify the Gemini model version\n",
        "    contents=\"INSERT YOUR PROMPT HERE\",  # The input prompt\n",
        "    config=types.GenerateContentConfig(\n",
        "        temperature=0.4,  # Controls randomness; lower values make responses more deterministic\n",
        "        top_p=0.95,  # Consider words until their combined probability reaches 95%\n",
        "        top_k=20,  # Limits token selection to the top 20 most likely next tokens\n",
        "        candidate_count=1,  # Number of candidate responses to generate (higher values generate multiple variations)\n",
        "        seed=5,  # Ensures reproducibility of results by using a fixed random seed\n",
        "        max_output_tokens=100,  # Limits the response length to 100 tokens\n",
        "        stop_sequences=[\"STOP!\"],  # Stops generation when this sequence appears in the response\n",
        "        presence_penalty=0.0,  # Encourages or discourages new topics (higher values penalize repetition)\n",
        "        frequency_penalty=0.0,  # Penalizes frequent token usage to reduce repetition\n",
        "    )\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMnbNpYwhegb"
      },
      "outputs": [],
      "source": [
        "# Generate a response with updated output paramaters\n",
        "response = client.models.generate_content(\n",
        "    model=model_id,\n",
        "    contents=\"Tell me how the internet works, but pretend I'm a puppy who only understands squeaky toys.\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        temperature=0.4,\n",
        "        top_p=0.95,\n",
        "        top_k=20,\n",
        "        candidate_count=1,\n",
        "        seed=5,\n",
        "        max_output_tokens=100,\n",
        "        stop_sequences=[\"STOP!\"],\n",
        "        presence_penalty=0.0,\n",
        "        frequency_penalty=0.0,\n",
        "    )\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWvJbj_913sS"
      },
      "source": [
        "### Adjusting Safety Filters\n",
        "\n",
        "Safety filters help you modulate the type of safety filters that are applied to the output. For example, if you are using Gemini to write content that can be deemed violent or dangerous for a fictional story, you may want to tone down the filters.\n",
        "\n",
        "- **Categories**: Define content types to filter (e.g., `HARM_CATEGORY_HARASSMENT`, `HARM_CATEGORY_HATE_SPEECH`, `HARM_CATEGORY_SEXUALLY_EXPLICIT`, `HARM_CATEGORY_DANGEROUS_CONTENT`, and `HARM_CATEGORY_CIVIC_INTEGRITY`).\n",
        "- **Thresholds**: Control how strictly content is filtered (e.g., `\"BLOCK_ONLY_HIGH\"` blocks high-severity cases).\n",
        "\n",
        "\n",
        "**Safety Threshold Levels**\n",
        "\n",
        "| Threshold (Google AI Studio) | Threshold (API)                         | Description |\n",
        "|------------------------------|------------------------------------------|-------------|\n",
        "| Block none                   | `BLOCK_NONE`                             | Always show regardless of probability of unsafe content |\n",
        "| Block few                    | `BLOCK_ONLY_HIGH`                        | Block when high probability of unsafe content |\n",
        "| Block some                   | `BLOCK_MEDIUM_AND_ABOVE`                 | Block when medium or high probability of unsafe content |\n",
        "| Block most                   | `BLOCK_LOW_AND_ABOVE`                    | Block when low, medium, or high probability of unsafe content |\n",
        "| N/A                          | `HARM_BLOCK_THRESHOLD_UNSPECIFIED`       | Threshold is unspecified, block using default threshold |\n",
        "\n",
        "We can update safety settings using the `types.SafetySetting()` function.\n",
        "\n",
        "\n",
        "```python\n",
        "safety_settings = [\n",
        "    types.SafetySetting(\n",
        "        category=\"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
        "        threshold=\"BLOCK_ONLY_HIGH\"\n",
        "    )\n",
        "]\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=types.GenerateContentConfig(),\n",
        "    safety_settings=safety_settings,\n",
        ")\n",
        "\n",
        "Markdown(response.text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgAjqkmbheiy"
      },
      "outputs": [],
      "source": [
        "# Remove dangerous content safety filter\n",
        "safety_settings = [\n",
        "    types.SafetySetting(\n",
        "        category=\"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
        "        threshold=\"BLOCK_NONE\",\n",
        "    ),\n",
        "]\n",
        "\n",
        "# Test the safety on a prompt of a fictional Batman setting\n",
        "prompt = \"\"\"\n",
        "    What are five punishment tactics that Batman can apply to the Joker after Joker did serious harm to Gotham.\n",
        "    List the tactics in bullet points and they should range from least violent to most violent.\n",
        "\"\"\"\n",
        "\n",
        "# Generate a response\n",
        "response = client.models.generate_content(\n",
        "    model=model_id,\n",
        "    contents=prompt,\n",
        "    config=types.GenerateContentConfig(\n",
        "        safety_settings=safety_settings,\n",
        "    ),\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kiiNpYZ3Xw2"
      },
      "source": [
        "### Multi-Turn Chat with Context\n",
        "\n",
        "We can create a conversational experience where responses build on previous interactions.  \n",
        "\n",
        "This is useful for:\n",
        "- Assistants applications that remember context.\n",
        "- Coding assistants that generate and refine code iteratively.\n",
        "\n",
        "We can create a chat instance using the `client.chats.create()` function and and the `.send_message()` method in the instantiated chat object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0wQ83_fhelK"
      },
      "outputs": [],
      "source": [
        "# Define a system instruction for the model\n",
        "system_instruction = \"\"\"\n",
        "You are an expert data scientist that can create effective code in Python.\n",
        "\"\"\"\n",
        "\n",
        "# Create a chat object\n",
        "chat = client.chats.create(\n",
        "    model=model_id,\n",
        "    config=types.GenerateContentConfig(\n",
        "        system_instruction=system_instruction,\n",
        "        temperature=0.4\n",
        "    )\n",
        ")\n",
        "\n",
        "# Sending messages to Gemini in a conversation flow\n",
        "response = chat.send_message(\"Write a function that cleans a text column from any dashes or spaces in pandas.\")\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlV2BOpR38LN"
      },
      "outputs": [],
      "source": [
        "# Follow up in context\n",
        "response = chat.send_message(\"Update that function to also remove any numbers.\")\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAi6E_wNiKmP"
      },
      "source": [
        "## Multimodal Capabilities of Gemini 2.0\n",
        "\n",
        "Gemini 2.0 is a **multimodal** model, meaning it can process and generate responses based on a variety of input formats, including **text, images, audio, video, and documents**. Moreoever, it has an XX context length for inputs, this makes it highly versatile for real-world applications. In this section, we will work with:\n",
        "\n",
        "- **Images**: Uploading and reasoning with images\n",
        "- **Text**: Uploading and reasoning with text files\n",
        "- **Audio**: Uploading and reasoning wiht audio files\n",
        "- **Documents**: Uploading and reasoning wiht PDF files\n",
        "- **Video**: Uploading and reasoning wiht video files\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htPqRPiS5w92"
      },
      "source": [
        "### Image Understanding with Gemini 2.0\n",
        "\n",
        "We can provide an image and ask Gemini 2.0 to describe or analyze it.\n",
        "\n",
        "#### Steps:\n",
        "1. Load an image using `PIL.Image`.\n",
        "2. Pass the image along with a text prompt to `generate_content_stream()`.\n",
        "3. Iterate through the streamed response and display the output.\n",
        "\n",
        "```python\n",
        "image = PIL.Image.open('image.png')\n",
        "\n",
        "response = client.models.generate_content_stream(\n",
        "    model=\"gemini-2.0-pro-exp-02-05\",\n",
        "    contents=[\"Explain the image\", image]\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    print(chunk.text, end=\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKz2T454iRKr"
      },
      "outputs": [],
      "source": [
        "# Open the image\n",
        "image = PIL.Image.open('/content/DeepSeek_R1.png')\n",
        "\n",
        "# Have the model explain the image\n",
        "response = client.models.generate_content_stream(\n",
        "    model=model_id,\n",
        "    contents=[\"Explain the image\", image])\n",
        "\n",
        "for chunk in response:\n",
        "    print(chunk.text, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNMQAD8_6bib"
      },
      "source": [
        "### Audio Understanding with Gemini 2.0\n",
        "\n",
        "Gemini 2.0 can process **audio files** and generate insights, transcriptions, or descriptions based on the content. This is useful for applications like:\n",
        "\n",
        "- **Speech-to-text transcription**\n",
        "- **Audio event recognition**\n",
        "- **Music or sound analysis**\n",
        "\n",
        "#### Steps:\n",
        "1. **Load the audio file** as binary data.\n",
        "2. **Wrap it in `types.Part.from_bytes()`** to inform Gemini that it's an audio file.\n",
        "3. **Provide a text prompt** to guide the model's response.\n",
        "4. **Process the streamed response** and print the output.\n",
        "\n",
        "```python\n",
        "with open('audio.wav', 'rb') as f:\n",
        "    audio_bytes = f.read()  # Read the audio file as bytes\n",
        "\n",
        "response = client.models.generate_content_stream(\n",
        "    model=model_id,\n",
        "    contents=[\n",
        "        \"Describe this audio\",\n",
        "        types.Part.from_bytes(\n",
        "            data=audio_bytes,\n",
        "            mime_type=\"audio/wav\",  # Specify the correct MIME type for audio\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Iterate over the streamed response and print the output\n",
        "for chunk in response:\n",
        "    print(chunk.text, end=\"\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkfIQi5yiRIT"
      },
      "outputs": [],
      "source": [
        "# Understand a DataFramed Podcast Episode\n",
        "with open('/content/DataFramed Episode_Meri Nova.mp3', 'rb') as f:\n",
        "    audio_bytes = f.read()\n",
        "\n",
        "response = client.models.generate_content_stream(\n",
        "  model=model_id,\n",
        "  contents=[\n",
        "    'Describe this audio',\n",
        "    types.Part.from_bytes(\n",
        "      data=audio_bytes,\n",
        "      mime_type='audio/wav',\n",
        "    )\n",
        "  ]\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    print(chunk.text, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JQ_Wk_h7z0w"
      },
      "source": [
        "### Document Understanding with Gemini 2.0\n",
        "\n",
        "Gemini 2.0 can process and analyze **documents** such as PDFs, Word files, and other text-based formats. This is useful for tasks like:\n",
        "\n",
        "- **Summarizing reports or research papers**\n",
        "- **Extracting key information from documents**\n",
        "- **Answering questions based on document content**\n",
        "\n",
        "### RAG vs Context Window\n",
        "\n",
        "In a lot of ways, Gemini removes the need for RAG on many document processing tasks. Here's why:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nd4v39vZ-RxH"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "display(Image(\"/content/Gemini Image_1.png\"))\n",
        "display(Image(\"/content/Gemini Image_2.png\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tB-vbJZ-8Gv"
      },
      "source": [
        "### Steps:\n",
        "1. **Load the document** as binary data.\n",
        "2. **Wrap it in `types.Part.from_bytes()`** to specify that it's a document.\n",
        "3. **Provide a text prompt** to guide the Gemini’s response.\n",
        "4. **Stream and display the output**.\n",
        "\n",
        "```python\n",
        "# Read the PDF file as binary data\n",
        "response = client.models.generate_content_stream(\n",
        "    model=model_id,\n",
        "    contents=[\n",
        "        \"What is this document about?\", # Provide the prompt\n",
        "        types.Part.from_bytes(\n",
        "            data=pathlib.Path(\"PATH TO FILE\").read_bytes(),  # Read the document\n",
        "            mime_type=\"application/pdf\",  # Specify the MIME type\n",
        "        )    \n",
        "    ]\n",
        ")\n",
        "\n",
        "# Stream the response and print it as it arrives\n",
        "for chunk in response:\n",
        "    print(chunk.text, end=\"\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oY7A_NIviRF7"
      },
      "outputs": [],
      "source": [
        "# Read the DeepSeek V3 paper and explain it\n",
        "response = client.models.generate_content_stream(\n",
        "  model=model_id,\n",
        "  contents=[\n",
        "      \"What is this document about?\",\n",
        "      types.Part.from_bytes(\n",
        "        data=pathlib.Path('/content/DeepSeek_V3.pdf').read_bytes(),\n",
        "        mime_type='application/pdf',\n",
        "      )]\n",
        "  )\n",
        "\n",
        "for chunk in response:\n",
        "    print(chunk.text, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2Xr81ApBC7n"
      },
      "source": [
        "### Video Understanding with Gemini 2.0\n",
        "\n",
        "Gemini 2.0 can process and analyze **videos** such as MP4s and .mov files, and other video-based formats.\n",
        "\n",
        "#### Steps:\n",
        "- Load the video and upload it to Gemini using `client.files.upload()`.\n",
        "- Load the video into `client.models.content()` with your prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pT9BezF4iRDV"
      },
      "outputs": [],
      "source": [
        "# Step 1: Upload the videos to Gemini\n",
        "\n",
        "import time\n",
        "\n",
        "def upload_video(video_file_name):\n",
        "  video_file = client.files.upload(file=video_file_name)\n",
        "\n",
        "  while video_file.state == \"PROCESSING\":\n",
        "      print('Waiting for video to be processed.')\n",
        "      time.sleep(10)\n",
        "      video_file = client.files.get(name=video_file.name)\n",
        "\n",
        "  if video_file.state == \"FAILED\":\n",
        "    raise ValueError(video_file.state)\n",
        "  print(f'Video processing complete: ' + video_file.uri)\n",
        "\n",
        "  return video_file\n",
        "\n",
        "chatgpt_tasks_video = upload_video('/content/ChatGPT Tasks.mp4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QMg5M6pA0Dk"
      },
      "outputs": [],
      "source": [
        "# Step 2: Reason about the video with Gemini\n",
        "response = client.models.generate_content(\n",
        "    model=model_id,\n",
        "    contents=[\n",
        "        chatgpt_tasks_video,\n",
        "        \"What is the content of this video?\",\n",
        "    ]\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmEBjgbpjMAr"
      },
      "source": [
        "## Putting It All together — Build a YouTube Worfklow App\n",
        "\n",
        "### What are we trying to build?\n",
        "\n",
        "A simple web app that allows to perform routine tasks related to YouTube content creation (e.g. summarizing chapter notes, create a title, provide ideas based on papers, etc..)\n",
        "\n",
        "### What will it look like?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFXxUdyokWAd"
      },
      "outputs": [],
      "source": [
        "display(Image(\"/content/Gemini Image_3.png\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djefFqwJExMF"
      },
      "source": [
        "### Steps needed to put it together\n",
        "\n",
        "- Define the logic of the video processing function\n",
        "- Define the logic of the PDF processing function\n",
        "- Build a gradio web-app"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0a-eIx-Gbxn"
      },
      "source": [
        "#### Step: 1 — Define the logic of the video processing function\n",
        "\n",
        "##### Step 1.A — Uploading and Processing Videos with Gemini 2.0\n",
        "\n",
        "This function **uploads a video file** to the Gemini API and **waits for processing** to complete. It works by:\n",
        "\n",
        "1. **Uploading the video** using `client.files.upload()`.\n",
        "2. **Polling the processing status** every 10 seconds.\n",
        "3. **Handling failures** by raising an error if processing fails.\n",
        "4. **Returning the processed file** once complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Yw-ta9ME76R"
      },
      "outputs": [],
      "source": [
        "def upload_video(video_file_path):\n",
        "    \"\"\"\n",
        "    Uploads a video file to the Gemini API and waits for processing.\n",
        "    \"\"\"\n",
        "\n",
        "    # Upload the video file to Gemini API\n",
        "    video_file = client.files.upload(file=video_file_path)\n",
        "\n",
        "    # Poll until processing is complete\n",
        "    while video_file.state == \"PROCESSING\":  # Check if the video is still being processed\n",
        "        print(\"Waiting for video to be processed...\")  # Notify the user\n",
        "        time.sleep(10)  # Wait for 10 seconds before checking again\n",
        "        video_file = client.files.get(name=video_file.name)  # Refresh the file status\n",
        "\n",
        "    # If processing fails, raise an error\n",
        "    if video_file.state == \"FAILED\":\n",
        "        raise ValueError(\"Video processing failed\")\n",
        "\n",
        "    # If processing is successful, print and return the processed video file URI\n",
        "    print(f\"Video processing complete: {video_file.uri}\")\n",
        "    return video_file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_jWpAyjGot4"
      },
      "source": [
        "##### Step 1.B — Process the video by Gemini\n",
        "\n",
        "This function allows **YouTube video processing** by:\n",
        "1. **Extracting video metadata** using `yt-dlp` (without downloading initially).\n",
        "2. **Downloading the video** using `wget`.\n",
        "3. **Uploading the video** to the **Gemini API** for processing.\n",
        "4. **Generating insights** based on a selected **prompt type**.\n",
        "\n",
        "**Available Prompts:**\n",
        "\n",
        "- **\"Generate Chapters\"** → Organizes video scenes into structured chapters.\n",
        "- **\"Generate Title & Description\"** → Suggests an SEO-friendly YouTube title and description."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Er0ge3xtE74L"
      },
      "outputs": [],
      "source": [
        "def process_youtube(youtube_url, prompt_choice):\n",
        "    \"\"\"\n",
        "    Given a YouTube URL and a selected prompt, this function:\n",
        "      1. Extracts video information using yt-dlp.\n",
        "      2. Downloads the video to a temporary file.\n",
        "      3. Uploads the video file to the Gemini API.\n",
        "      4. Calls Gemini to generate content based on the selected prompt.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define yt-dlp options to extract video metadata without downloading\n",
        "    ydl_opts = {\n",
        "        'format': 'best',      # Get the best quality available\n",
        "        'quiet': True,         # Suppress unnecessary output\n",
        "        'skip_download': True, # Only extract info, do not download\n",
        "    }\n",
        "\n",
        "    # Extract video metadata (title, direct video URL, etc.)\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        info_dict = ydl.extract_info(youtube_url, download=False)\n",
        "\n",
        "    # Generate a filename for the downloaded video\n",
        "    output_filename = str(info_dict['fulltitle']) + '.mp4'\n",
        "    print(f\"\\nDownloading video to {output_filename} ...\")\n",
        "\n",
        "    # Get the direct video URL (handles cases where 'url' may not be available)\n",
        "    direct_url = info_dict.get('url', info_dict['formats'][0]['url'])\n",
        "    print(\"Direct video URL extracted:\", direct_url)\n",
        "\n",
        "    # Download the video using wget\n",
        "    print(f\"Downloading video to {output_filename} ...\")\n",
        "    wget.download(direct_url, out=output_filename)\n",
        "    print(f\"\\nDownloaded video to {output_filename}\")\n",
        "\n",
        "    # Upload video to Gemini API and wait for processing\n",
        "    youtube_video = upload_video(output_filename)\n",
        "\n",
        "    # Remove the temporary video file after upload\n",
        "    os.remove(output_filename)\n",
        "\n",
        "    # Define a dictionary mapping prompt choices to specific AI tasks\n",
        "    prompt_options = {\n",
        "        \"Generate Chapters\": \"\"\"Organize all scenes from this video into chapters and chapter markers,\n",
        "                             output your response as bullet points where each bullet is a chapter marker\n",
        "                             and the title of the chapter.\"\"\",\n",
        "        \"Generate Title & Description\": \"\"\"Provide one short YouTube title and description for this video —\n",
        "                                        that is both accurate, engaging, and SEO-friendly.\"\"\"\n",
        "    }\n",
        "\n",
        "    # Get the corresponding prompt text based on user selection; defaults to \"Generate Title & Description\"\n",
        "    prompt_text = prompt_options.get(prompt_choice, \"Generate Title & Description\")\n",
        "\n",
        "    # Generate content using Gemini API with the uploaded video and selected prompt\n",
        "    response = client.models.generate_content(\n",
        "        model=model_id,\n",
        "        contents=[youtube_video, prompt_text],\n",
        "        config=types.GenerateContentConfig(\n",
        "            max_output_tokens=100)\n",
        "    )\n",
        "\n",
        "    return response.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gyMaPL9OuOx"
      },
      "outputs": [],
      "source": [
        "# Test out the functions before setting it up in a gradio application\n",
        "prompt_choice = \"Generate Title & Description\" # @param  [\"Generate Chapters\", \"Generate Title & Description\"]\n",
        "youtube_url = \"https://www.youtube.com/watch?v=4j8OOH7x2Ho\"\n",
        "\n",
        "process_youtube(youtube_url, prompt_choice)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLKMXGsUHpTH"
      },
      "source": [
        "#### Step: 2 — Define the logic of the PDF processing function\n",
        "\n",
        "\n",
        "This function allows **PDF document analysis** using the **Gemini API** by:\n",
        "\n",
        "1. **Reading the PDF file** as binary data.\n",
        "2. **Selecting a predefined prompt** to guide AI-generated content.\n",
        "3. **Streaming responses text** based on the document’s content.\n",
        "\n",
        "**Available Prompts:**\n",
        "\n",
        "- **\"YouTube Script\"** → Generates an engaging video script from the document.\n",
        "- **\"Summarize Document\"** → Provides a concise summary of the document.\n",
        "- **\"Video Ideas\"** → Suggests YouTube video topics inspired by the document.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiAzbFr6E71z"
      },
      "outputs": [],
      "source": [
        "def process_pdf(pdf_file, prompt_choice):\n",
        "    \"\"\"\n",
        "    Given a PDF file and a selected prompt, this function:\n",
        "      1. Reads the PDF file.\n",
        "      2. Selects the appropriate prompt text.\n",
        "      3. Streams the content generation using the Gemini API.\n",
        "    \"\"\"\n",
        "\n",
        "    # Determine the file path (Handle both Gradio file object and direct string paths)\n",
        "    if isinstance(pdf_file, dict) and \"name\" in pdf_file:\n",
        "        file_path = pdf_file[\"name\"]  # Gradio file object\n",
        "    elif isinstance(pdf_file, str):  # Direct path in normal execution\n",
        "        file_path = pdf_file\n",
        "    else:\n",
        "        return \"Invalid PDF file input.\"\n",
        "\n",
        "    # Read the PDF file as bytes\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        pdf_bytes = f.read()\n",
        "\n",
        "    # Define a dictionary mapping prompt choices to specific prompts\n",
        "    prompt_options = {\n",
        "        \"YouTube Script\": \"Create an engaging YouTube script based on this document.\",\n",
        "        \"Summarize Document\": \"Summarize this document for me.\",\n",
        "        \"Video Ideas\": \"What are YouTube video ideas I can pursue based off of this document?\"\n",
        "    }\n",
        "\n",
        "    # Get the corresponding prompt text based on user selection; defaults to \"YouTube Script\"\n",
        "    prompt_text = prompt_options.get(prompt_choice, \"Create an engaging YouTube script based on this document.\")\n",
        "\n",
        "    # Call the Gemini API in streaming mode, sending the PDF content and prompt\n",
        "    response_stream = client.models.generate_content_stream(\n",
        "        model=model_id,\n",
        "        contents=[\n",
        "            types.Part.from_bytes(\n",
        "                data=pdf_bytes,              # Attach the PDF file as input\n",
        "                mime_type='application/pdf'  # Specify the MIME type for PDFs\n",
        "            ),\n",
        "            prompt_text  # The selected prompt guiding the response\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Collect all streamed chunks into a single response string\n",
        "    full_response = \"\"\n",
        "    for chunk in response_stream:\n",
        "        full_response += chunk.text  # Append each chunk to the final response\n",
        "\n",
        "    return full_response  # Return the generated response as a complete text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gq5IPRtON0X"
      },
      "outputs": [],
      "source": [
        "## Test out the functions before setting it up in a gradio application\n",
        "prompt_choice = \"YouTube Script\" # @param  [\"YouTube Script\", \"Summarize Document\", \"Video Ideas\"]\n",
        "pdf_file = \"/content/DeepSeek_V3.pdf\"\n",
        "\n",
        "process_pdf(pdf_file, prompt_choice)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78aRt4CPH2uy"
      },
      "source": [
        "#### Step: 3 — Define the Gradio Application\n",
        "\n",
        "This Gradio application allows you to **process PDFs and YouTube videos** using **Gemini 2.0** according to a set of prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhHSXpA4k7hJ"
      },
      "outputs": [],
      "source": [
        "with gr.Blocks() as demo:\n",
        "    # Title and description for the application\n",
        "    gr.Markdown(\"# Gemini API Code-Along Application\")\n",
        "    gr.Markdown(\"Select a tab to process either a PDF file or a YouTube video using different prompts.\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "        # --- PDF Processing Tab ---\n",
        "        with gr.Tab(\"PDF Processing\"):\n",
        "            gr.Markdown(\"**Upload a PDF file and select a prompt.**\")\n",
        "\n",
        "            # File uploader for PDF files\n",
        "            pdf_input = gr.File(label=\"Upload PDF\", file_types=[\".pdf\"])\n",
        "\n",
        "            # Radio button to select a processing prompt for the PDF\n",
        "            pdf_prompt = gr.Radio(\n",
        "                choices=[\"YouTube Script\", \"Summarize Document\", \"Video Ideas\"],\n",
        "                label=\"Select PDF Prompt\",\n",
        "                value=\"YouTube Script\"  # Default selected option\n",
        "            )\n",
        "\n",
        "            # Textbox to display the generated response\n",
        "            pdf_output = gr.Textbox(label=\"Response\", lines=10)\n",
        "\n",
        "            # Button to trigger PDF processing\n",
        "            pdf_button = gr.Button(\"Process PDF\")\n",
        "\n",
        "            # Clicking the button triggers the `process_pdf` function\n",
        "            pdf_button.click(fn=process_pdf, inputs=[pdf_input, pdf_prompt], outputs=pdf_output)\n",
        "\n",
        "        # --- YouTube Processing Tab ---\n",
        "        with gr.Tab(\"YouTube Processing\"):\n",
        "            gr.Markdown(\"**Enter a YouTube video URL and select a prompt.**\")\n",
        "\n",
        "            # Textbox for entering YouTube video URL\n",
        "            youtube_url_input = gr.Textbox(label=\"YouTube Video URL\")\n",
        "\n",
        "            # Radio button to select a processing prompt for the YouTube video\n",
        "            youtube_prompt = gr.Radio(\n",
        "                choices=[\"Generate Chapters\", \"Generate Title & Description\"],\n",
        "                label=\"Select YouTube Prompt\",\n",
        "                value=\"Generate Chapters\"  # Default selected option\n",
        "            )\n",
        "\n",
        "            # Textbox to display the generated response\n",
        "            youtube_output = gr.Textbox(label=\"Response\", lines=10)\n",
        "\n",
        "            # Button to trigger YouTube video processing\n",
        "            youtube_button = gr.Button(\"Process YouTube Video\")\n",
        "\n",
        "            # Clicking the button triggers the `process_youtube` function\n",
        "            youtube_button.click(fn=process_youtube, inputs=[youtube_url_input, youtube_prompt], outputs=youtube_output)\n",
        "\n",
        "\n",
        "# Enable the queue to handle long-running functions and improve responsiveness\n",
        "demo.queue()\n",
        "\n",
        "# Launch the Gradio app with debugging enabled\n",
        "demo.launch(debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vr4ncnarIZhL"
      },
      "outputs": [],
      "source": [
        "display(Image(\"/content/Gemini Image_4.png\", width = 800))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}